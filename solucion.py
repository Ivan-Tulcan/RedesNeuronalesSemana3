# %% [markdown]
# # üß† Soluci√≥n Tarea Semana 3: Visualizaci√≥n de Mapas de Activaci√≥n en CNN con MNIST
# 
# ## üéØ Objetivo
# Entrenar una red neuronal convolucional (CNN) para clasificar im√°genes de d√≠gitos escritos a mano (MNIST), y visualizar qu√© caracter√≠sticas aprende cada capa mediante mapas de activaci√≥n.
# 
# ---

# %% [markdown]
# ## Setup inicial (importe de m√≥dulos)

# %%
import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {tf.keras.__version__}")

# %%
# Cargar y preparar los datos MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalizar los valores de p√≠xeles al rango [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Agregar dimensi√≥n de canal (escala de grises = 1 canal)
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

print(f"Forma de x_train: {x_train.shape}")
print(f"Forma de y_train: {y_train.shape}")
print(f"Forma de x_test: {x_test.shape}")
print(f"Forma de y_test: {y_test.shape}")

# Visualizar algunas im√°genes de ejemplo
plt.figure(figsize=(10, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(f'Digit: {y_train[i]}')
    plt.axis('off')
plt.suptitle('Ejemplos del dataset MNIST')
plt.tight_layout()
plt.show()

# %% [markdown]
# ## üì¶ Parte 1: Definici√≥n y entrenamiento del modelo
# 
# ### Definici√≥n de variables para la arquitectura

# %%
# par√°metros de la arquitectura
alto = 28  # altura de la imagen
ancho = 28  # ancho de la imagen
clases = 10  # n√∫mero de clases (d√≠gitos 0-9)

print(f"Dimensiones de entrada: {alto} x {ancho}")
print(f"N√∫mero de clases: {clases}")

# %%
# Definir la entrada de la red 
inputs = Input(shape=(alto, ancho, 1))

# Primer bloque convolucional:
x = layers.Conv2D(8, (3, 3), activation='relu', name='conv1')(inputs)
x = layers.MaxPooling2D((2, 2))(x)  

# Segundo bloque convolucional: 
x = layers.Conv2D(16, (3, 3), activation='relu', name='conv2')(x)
x = layers.MaxPooling2D((2, 2))(x)

# Capa densa para clasificaci√≥n final
x = layers.Flatten()(x)
x = layers.Dense(64, activation='relu')(x)
outputs = layers.Dense(clases, activation='softmax')(x)

# Compilacion
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Resumen del modelo
print(model.summary())

# %%
# Entrenar el modelo por 5 √©pocas
print("Iniciando entrenamiento...")
history = model.fit(x_train, y_train, epochs=5, validation_split=0.1, verbose=1)
print("\nEntrenamiento completado!")

# %% [markdown]
# ### Preguntas Pt. 1 
# 
# **1. ¬øPor qu√© usamos una entrada de tama√±o `(28, 28, 1)`? ¬øQu√© representa cada dimensi√≥n?**
# 
# **Respuesta:** Dimensiones:`(28, 28, 1)`:
# - 28: Altura de la imagen en p√≠xeles
# - 28: Ancho de la imagen en p√≠xeles  
# - 1: N√∫mero de canales (escala de grises, solo un canal vs RGB que tendr√≠a 3)
# 
# **2. En la primera capa convolucional usamos 8 filtros de tama√±o 3√ó3. ¬øQu√© significan estos filtros?**
# 
# **Respuesta:** Los filtros son matrices de 3√ó3 que se deslizan por la imagen para detectar las caracter√≠sticas b√°sicas (por ejemplo, bordes horizontales, verticales, diagonales, esquinas, etc.) Cada filtro aprende a detectar un patr√≥n espec√≠fico.
# 
# **3. ¬øQu√© efecto tiene `MaxPooling2D` sobre la salida de la convoluci√≥n?**
# 
# **Respuesta:** El maxpooling2d va a reducir las dimensiones espaciales a la mitad tomando el valor m√°ximo de cada ventana 2√ó2.
# - Reduce el tama√±o de los datos
# - Conserva las caracter√≠sticas m√°s importantes
# - Agrega invariancia a peque√±as traslaciones
# - Reduce el costo computacional
# 
# **4. ¬øPor qu√© la √∫ltima capa tiene 10 neuronas y qu√© significa la funci√≥n `softmax` en este contexto?**
# 
# **Respuesta:** 
# - Una neurona por cada clase. (d√≠gitos 0-9)
# - Softmax convierte las salidas en probabilidades que suman 1, representando la confianza del modelo para cada clase
# 
# **5. Observa el `model.summary()` y explica:**

# %% [markdown]
# Entrada: (28, 28, 1) - Imagen original
# Conv1: (26, 26, 8) - Tras convoluci√≥n 3x3, se pierden 2 p√≠xeles por lado
# MaxPool1: (13, 13, 8) - Tras pooling 2x2, dimensiones se reducen a la mitad
# Conv2: (11, 11, 16) - Tras segunda convoluci√≥n 3x3
# MaxPool2: (5, 5, 16) - Tras segundo pooling 2x2
# Flatten: (400,) - 5√ó5√ó16 = 400 caracter√≠sticas lineales
# Dense1: (64,) - Capa densa intermedia
# Dense2: (10,) - Salida final con probabilidades para cada clase
# 
# ### ‚Ä¢ ¬øC√≥mo cambia el tama√±o del tensor?
#   Las dimensiones espaciales (altura√óancho) disminuyen progresivamente
#   mientras que la profundidad (canales) aumenta.
# 
# ### ‚Ä¢ ¬øPor qu√© disminuyen las dimensiones espaciales?
#   - Convoluciones sin padding reducen el tama√±o
#   - MaxPooling reduce las dimensiones a la mitad
#   - Esto permite capturar patrones en diferentes escalas
# 
# ### ‚Ä¢ ¬øPor qu√© aumenta el n√∫mero de filtros?
#   - Primeras capas: detectan caracter√≠sticas simples (pocos filtros)
#   - Capas m√°s profundas: combinan caracter√≠sticas para patrones complejos (m√°s filtros)
#   - Permite representar jerarqu√≠as de caracter√≠sticas

# %% [markdown]
# ## üîç Parte 2: Visualizaci√≥n de mapas de activaci√≥n

# %%
# Se selecciona una imagen del conjunto de prueba
img_index = 0
img = x_test[img_index:img_index+1]
true_label = y_test[img_index]

# Se muestra la imagen
plt.figure(figsize=(4, 4))
plt.imshow(img[0].reshape(28, 28), cmap='gray')
plt.title(f'Imagen seleccionada - D√≠gito: {true_label}')
plt.axis('off')
plt.show()

print(f"Forma de la imagen: {img.shape}")

# %%
# Se crea un modelo que devuelva las salidas intermedias de las capas convolucionales
layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]
feature_model = models.Model(inputs=model.input, outputs=layer_outputs)

print("Capas convolucionales encontradas:")
for i, layer in enumerate(model.layers):
    if 'conv' in layer.name:
        print(f"  {i}: {layer.name} - Salida: {layer.output.shape}")

# Obtiene los mapas de caracter√≠sticas
feature_maps = feature_model.predict(img)
print(f"\nN√∫mero de capas convolucionales: {len(feature_maps)}")
for i, fmap in enumerate(feature_maps):
    print(f"Capa {i+1}: {fmap.shape}")

# %%
# Visualiza los mapas de cada capa
for i, fmap in enumerate(feature_maps):
    num_filters = fmap.shape[-1]
    
    # Calcula el n√∫mero de filas necesarias
    cols = min(8, num_filters)  # M√°ximo 8 columnas
    rows = (num_filters + cols - 1) // cols 
    
    plt.figure(figsize=(16, rows * 2))
    
    for j in range(num_filters):
        plt.subplot(rows, cols, j+1)
        plt.imshow(fmap[0, :, :, j], cmap='viridis')
        plt.title(f'Filtro {j+1}')
        plt.axis('off')
    
    plt.suptitle(f"Mapas de Activaci√≥n - Capa Convolucional {i+1} ({num_filters} filtros)", fontsize=16)
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ### üß† Pregunta (Parte 2)
# 
# **Describe lo que observas en los mapas de caracter√≠sticas:**

# %% [markdown]
# # AN√ÅLISIS DE MAPAS DE ACTIVACI√ìN
# 
# ## PRIMERA CAPA CONVOLUCIONAL (Conv1):
# ‚Ä¢ ¬øLa primera capa detecta bordes o contornos?
#   S√ç - La primera capa detecta caracter√≠sticas b√°sicas como:
#   - Bordes horizontales y verticales
#   - Contornos y l√≠neas
#   - Cambios de intensidad en diferentes orientaciones
#   - Cada filtro se especializa en detectar un tipo espec√≠fico de borde
# 
# ## SEGUNDA CAPA CONVOLUCIONAL (Conv2):
# ‚Ä¢ ¬øLa segunda capa comienza a detectar formas m√°s complejas?
#   S√ç - La segunda capa combina los bordes de la primera capa para detectar:
#   - Esquinas y √°ngulos
#   - Curvas y bucles
#   - Patrones m√°s complejos que forman partes de d√≠gitos
#   - Combinaciones de l√≠neas que forman formas caracter√≠sticas
# 
# ## REDUCCI√ìN DE TAMA√ëO CON MaxPooling:
# ‚Ä¢ ¬øQu√© tanto se reduce la imagen con las capas de MaxPooling?
#   - Imagen original: 28√ó28 p√≠xeles
#   - Despu√©s del primer MaxPooling: 13√ó13 p√≠xeles (~75% de reducci√≥n en √°rea)
#   - Despu√©s del segundo MaxPooling: 5√ó5 p√≠xeles (~97% de reducci√≥n total)
#   - Cada MaxPooling reduce las dimensiones espaciales a la mitad
#   - Se conservan las caracter√≠sticas m√°s importantes (valores m√°ximos)
# 
# ## INTERPRETACI√ìN:
# - Jerarqu√≠a de caracter√≠sticas: simple ‚Üí compleja
# - Reducci√≥n progresiva del tama√±o espacial
# - Aumento en la abstracci√≥n de las caracter√≠sticas detectadas
# - Los filtros aprenden autom√°ticamente qu√© patrones son √∫tiles para clasificar d√≠gitos

# %% [markdown]
# ## üìä Parte 3: Evaluaci√≥n del modelo

# %%
# Evaluar el modelo en el conjunto de prueba
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Precisi√≥n en el conjunto de prueba: {test_accuracy:.4f}")
print(f"P√©rdida en el conjunto de prueba: {test_loss:.4f}")

# %%
# Predecir clases
y_pred_probs = model.predict(x_test, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)  # Convertir probabilidades en etiquetas

print(f"Forma de predicciones: {y_pred_probs.shape}")
print(f"Primeras 10 predicciones: {y_pred[:10]}")
print(f"Primeras 10 etiquetas reales: {y_test[:10]}")

# %%
# Matriz de confusi√≥n
plt.figure(figsize=(10, 8))
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred,
    cmap='Blues',
    colorbar=True,
    display_labels=np.arange(10)
)
plt.title("Matriz de Confusi√≥n - Clasificaci√≥n MNIST", fontsize=16)
plt.grid(False)
plt.tight_layout()
plt.show()

# %%
# Reporte de m√©tricas por clase
print("=== REPORTE DE CLASIFICACI√ìN ===")
print()
print(classification_report(y_test, y_pred, digits=4))

# %% [markdown]
# ### üß† Preguntas (Parte 3)

# %%
# An√°lisis detallado de los resultados
from sklearn.metrics import confusion_matrix

# Calcular matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)

print("=== AN√ÅLISIS DE RESULTADOS ===")
print()

# 1. D√≠gitos m√°s f√°ciles de clasificar
diagonal = np.diag(cm)
totals = np.sum(cm, axis=1)
accuracies = diagonal / totals

print("1. ¬øQu√© d√≠gitos fueron m√°s f√°ciles de clasificar?")
best_digits = np.argsort(accuracies)[::-1][:3]
for digit in best_digits:
    acc = accuracies[digit]
    print(f"   D√≠gito {digit}: {acc:.4f} ({acc*100:.2f}%)")
print()

# 2. D√≠gitos con m√°s errores
print("2. ¬øD√≥nde se cometieron m√°s errores? ¬øEn qu√© clases?")
worst_digits = np.argsort(accuracies)[:3]
for digit in worst_digits:
    acc = accuracies[digit]
    errors = totals[digit] - diagonal[digit]
    print(f"   D√≠gito {digit}: {errors} errores, precisi√≥n {acc:.4f} ({acc*100:.2f}%)")
print()

# Analizar confusiones m√°s comunes
print("   Confusiones m√°s comunes (errores > 5):")
for i in range(10):
    for j in range(10):
        if i != j and cm[i][j] > 5:
            print(f"   {cm[i][j]} veces: {i} clasificado como {j}")
print()

# 3. M√©tricas importantes
from sklearn.metrics import precision_recall_fscore_support
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)

print("3. ¬øQu√© m√©trica destacar√≠as y por qu√©?")
print(f"   ‚Ä¢ F1-Score promedio: {np.mean(f1):.4f}")
print(f"   ‚Ä¢ Precisi√≥n promedio: {np.mean(precision):.4f}")
print(f"   ‚Ä¢ Recall promedio: {np.mean(recall):.4f}")
print()
print("   DESTACAR√çA el F1-Score porque:")
print("   - Combina precisi√≥n y recall en una sola m√©trica")
print("   - Es √∫til cuando las clases est√°n balanceadas (como en MNIST)")
print("   - Nos da una medida general del rendimiento del modelo")
print()


# %% [markdown]
# ## üéØ Reflexi√≥n Final y Conclusiones

# %% [markdown]
# ## ¬øConsideras que este modelo convolucional es adecuado para el problema?
#    Si
#    - Logra una precisi√≥n muy alta (>98%)
#    - Las CNN son ideales para reconocimiento de im√°genes
#    - Detecta autom√°ticamente jerarqu√≠as de caracter√≠sticas
#    - Es computacionalmente eficiente para este tama√±o de problema
# 
# ## ¬øQu√© modificaciones propondr√≠as para mejorar el rendimiento?
#    - Data Augmentation: rotaciones, traslaciones, ruido
#    - M√°s capas convolucionales para mayor profundidad
#    - Batch Normalization para estabilizar el entrenamiento
#    - Dropout para reducir overfitting
#    - Learning rate scheduling
#    - M√°s √©pocas de entrenamiento
# 
# ## ¬øC√≥mo se compara con una red neuronal multicapa (MLP) tradicional?
#    VENTAJAS de CNN sobre MLP:
#    - Invariancia a traslaciones
#    - Menos par√°metros (pesos compartidos)
#    - Detecta patrones locales autom√°ticamente
#    - Preserva informaci√≥n espacial
#    - Mejor generalizaci√≥n para im√°genes
# 
#    3. La primera capa detecta bordes, las siguientes formas m√°s complejas
#    4. MaxPooling reduce dimensionalidad conservando informaci√≥n importante
#    5. La visualizaci√≥n ayuda a interpretar y debuggear modelos
#    6. MNIST es un problema bien resuelto por CNN b√°sicas

# %%
# Guardar algunos ejemplos de predicciones incorrectas para an√°lisis
incorrect_indices = np.where(y_pred != y_test)[0]
print(f"Total de predicciones incorrectas: {len(incorrect_indices)}")

# Mostrar algunos ejemplos de errores
if len(incorrect_indices) > 0:
    plt.figure(figsize=(15, 6))
    num_examples = min(8, len(incorrect_indices))
    
    for i in range(num_examples):
        idx = incorrect_indices[i]
        plt.subplot(2, 4, i+1)
        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')
        plt.title(f'Real: {y_test[idx]}, Pred: {y_pred[idx]}\nConf: {y_pred_probs[idx][y_pred[idx]]:.3f}')
        plt.axis('off')
    
    plt.suptitle('Ejemplos de Predicciones Incorrectas', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    print("\nObservaciones sobre los errores:")
    print("‚Ä¢ Algunos d√≠gitos pueden ser ambiguos incluso para humanos")
    print("‚Ä¢ La calidad de escritura afecta la clasificaci√≥n")
    print("‚Ä¢ D√≠gitos muy similares (como 4 y 9, o 3 y 8) se confunden m√°s")
else:
    print("¬°Perfecto! No hay errores de clasificaci√≥n.")


